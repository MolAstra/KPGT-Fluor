{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold\n",
    "from loguru import logger\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from typing import List, Tuple\n",
    "from typing import Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_dir = Path(\"../datasets/raw\")\n",
    "\n",
    "data_consolidation = pd.read_csv(raw_data_dir / \"Dataset_Consolidation_canonicalized.csv\")\n",
    "data_cyanine = pd.read_csv(raw_data_dir / \"Dataset_Cyanine_canonicalized.csv\")\n",
    "data_xanthene = pd.read_csv(raw_data_dir / \"Dataset_Xanthene_canonicalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 17:19:56.883\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdrop_duplicates\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mbefore dropping duplicates: 36750 rows\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 17:19:56.909\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdrop_duplicates\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mafter dropping duplicates: 36735 rows\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:56.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdrop_duplicates\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mbefore dropping duplicates: 1496 rows\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:56.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdrop_duplicates\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mafter dropping duplicates: 1496 rows\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:56.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdrop_duplicates\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mbefore dropping duplicates: 1152 rows\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:56.918\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdrop_duplicates\u001b[0m:\u001b[36m5\u001b[0m - \u001b[1mafter dropping duplicates: 1146 rows\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def drop_duplicates(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "    logger.info(f\"before dropping duplicates: {df.shape[0]} rows\")\n",
    "    df = df.drop_duplicates(subset=columns)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    logger.info(f\"after dropping duplicates: {df.shape[0]} rows\")\n",
    "    return df\n",
    "\n",
    "data_consolidation = drop_duplicates(data_consolidation, [\"smiles\", \"solvent\"])\n",
    "data_cyanine = drop_duplicates(data_cyanine, [\"smiles\", \"solvent\"])\n",
    "data_xanthene = drop_duplicates(data_xanthene, [\"smiles\", \"solvent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(\n",
    "    df: pd.DataFrame, save_dir: Path, name: str, random_state: int = 42\n",
    ") -> pd.DataFrame:\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    _save_dir = save_dir / \"random\"\n",
    "    _save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for fold, (train_index, valid_index) in enumerate(kf.split(df)):\n",
    "        _df = df.copy()\n",
    "        _df.loc[valid_index, \"split\"] = \"valid\"  # 这一折作为 valid\n",
    "        _df.loc[train_index, \"split\"] = \"train\"  # 剩余的数据作为 train\n",
    "        df_test = _df[_df[\"split\"] == \"valid\"].copy()\n",
    "        df_test[\"split\"] = \"test\"\n",
    "        _df = pd.concat([_df, df_test], ignore_index=True)\n",
    "        _df.to_csv(_save_dir / f\"{name}_fold{fold}.csv\", index=False)\n",
    "\n",
    "        n_total = len(_df)\n",
    "        n_test = len(_df[_df[\"split\"] == \"test\"])\n",
    "        n_valid = len(_df[_df[\"split\"] == \"valid\"])\n",
    "        n_train = len(_df[_df[\"split\"] == \"train\"])\n",
    "\n",
    "        logger.info(\n",
    "            f\"length of {name}_fold{fold}: {n_total}; length of train: {n_train}; length of valid: {n_valid}; length of test: {n_test}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 17:19:57.252\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of consolidation_fold0: 44082; length of train: 29388; length of valid: 7347; length of test: 7347\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:57.495\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of consolidation_fold1: 44082; length of train: 29388; length of valid: 7347; length of test: 7347\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:57.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of consolidation_fold2: 44082; length of train: 29388; length of valid: 7347; length of test: 7347\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:57.866\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of consolidation_fold3: 44082; length of train: 29388; length of valid: 7347; length of test: 7347\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of consolidation_fold4: 44082; length of train: 29388; length of valid: 7347; length of test: 7347\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of cyanine_fold0: 1796; length of train: 1196; length of valid: 300; length of test: 300\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.081\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of cyanine_fold1: 1795; length of train: 1197; length of valid: 299; length of test: 299\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.100\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of cyanine_fold2: 1795; length of train: 1197; length of valid: 299; length of test: 299\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.121\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of cyanine_fold3: 1795; length of train: 1197; length of valid: 299; length of test: 299\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of cyanine_fold4: 1795; length of train: 1197; length of valid: 299; length of test: 299\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of xanthene_fold0: 1376; length of train: 916; length of valid: 230; length of test: 230\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of xanthene_fold1: 1375; length of train: 917; length of valid: 229; length of test: 229\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.196\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of xanthene_fold2: 1375; length of train: 917; length of valid: 229; length of test: 229\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.214\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of xanthene_fold3: 1375; length of train: 917; length of valid: 229; length of test: 229\u001b[0m\n",
      "\u001b[32m2025-04-05 17:19:58.232\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mrandom_split\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mlength of xanthene_fold4: 1375; length of train: 917; length of valid: 229; length of test: 229\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "random_split(data_consolidation, raw_data_dir, \"consolidation\")\n",
    "random_split(data_cyanine, raw_data_dir, \"cyanine\")\n",
    "random_split(data_xanthene, raw_data_dir, \"xanthene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scaffold(smiles, include_chirality=False):\n",
    "    \"\"\"\n",
    "    Obtain Bemis-Murcko scaffold from smiles\n",
    "    :param smiles:\n",
    "    :param include_chirality:\n",
    "    :return: smiles of scaffold\n",
    "    \"\"\"\n",
    "    scaffold = MurckoScaffold.MurckoScaffoldSmiles(\n",
    "        smiles=smiles, includeChirality=include_chirality\n",
    "    )\n",
    "    return scaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_split(\n",
    "    smiles_list: List[str],\n",
    "    k: int = 5,  # Number of folds\n",
    "    balanced: bool = True,\n",
    "    seed: int = 42,\n",
    ") -> List[Tuple[List[int], List[int]]]:\n",
    "\n",
    "    # Generate scaffold-based dictionary\n",
    "    all_scaffolds = {}\n",
    "    scaffolds = []\n",
    "    for i, smiles in enumerate(tqdm(smiles_list)):\n",
    "        try:\n",
    "            scaffold = generate_scaffold(smiles, include_chirality=True)\n",
    "            scaffolds.append(scaffold)\n",
    "        except Exception:\n",
    "            logger.warning(f\"Error generating scaffold for {smiles}\")\n",
    "            continue\n",
    "        if scaffold not in all_scaffolds:\n",
    "            all_scaffolds[scaffold] = [i]\n",
    "        else:\n",
    "            all_scaffolds[scaffold].append(i)\n",
    "\n",
    "    # Group scaffolds into a list of index sets\n",
    "    scaffold_sets = list(all_scaffolds.values())\n",
    "\n",
    "    # If balancing is enabled, shuffle larger scaffold sets differently\n",
    "    if balanced:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(scaffold_sets)\n",
    "\n",
    "    # Create the KFold splits\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    folds = []\n",
    "\n",
    "    # We are going to assign each scaffold to one of the k folds\n",
    "    for train_idx, val_idx in kf.split(scaffold_sets):\n",
    "        train_fold = []\n",
    "        val_fold = []\n",
    "\n",
    "        # Collect indices for train and validation folds based on scaffolds\n",
    "        for idx in train_idx:\n",
    "            train_fold.extend(scaffold_sets[idx])\n",
    "        for idx in val_idx:\n",
    "            val_fold.extend(scaffold_sets[idx])\n",
    "\n",
    "        folds.append((train_fold, val_fold))\n",
    "\n",
    "    return folds, scaffolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaffold_split_df(\n",
    "    df: pd.DataFrame,\n",
    "    name: str,\n",
    "    k: int = 5,\n",
    "    balanced: bool = True,\n",
    "    seed: int = 42,\n",
    "    save_dir: Path = raw_data_dir,\n",
    ") -> List[Tuple[List[int], List[int]]]:\n",
    "    smiles_list = df[\"smiles\"].tolist()\n",
    "    folds, scaffolds = scaffold_split(smiles_list, k=k, balanced=balanced, seed=seed)\n",
    "    df[\"scaffold\"] = scaffolds\n",
    "\n",
    "    _save_dir = save_dir / \"scaffold\"\n",
    "    _save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n_scaffolds = df[\"scaffold\"].nunique()\n",
    "    n_smiles_unique = df[\"smiles\"].nunique()\n",
    "    logger.info(f\"number of scaffolds: {n_scaffolds}\")\n",
    "    logger.info(f\"number of smiles: {n_smiles_unique}\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(folds):\n",
    "        _df = df.copy()\n",
    "        _df.loc[val_idx, \"split\"] = \"valid\"\n",
    "        _df.loc[train_idx, \"split\"] = \"train\"\n",
    "        df_test = _df[_df[\"split\"] == \"valid\"].copy()\n",
    "        df_test[\"split\"] = \"test\"\n",
    "        _df = pd.concat([_df, df_test], ignore_index=True)\n",
    "        _df.to_csv(_save_dir / f\"{name}_fold{fold}.csv\", index=False)\n",
    "\n",
    "        n_total = len(_df)\n",
    "        n_test = len(_df[_df[\"split\"] == \"test\"])\n",
    "        n_valid = len(_df[_df[\"split\"] == \"valid\"])\n",
    "        n_train = len(_df[_df[\"split\"] == \"train\"])\n",
    "\n",
    "        logger.info(\n",
    "            f\"length of {name}_fold{fold}: {n_total}; length of train: {n_train}; length of valid: {n_valid}; length of test: {n_test}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df43158943fa48fc86fd6ae1f5a861e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 17:20:55.832\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mnumber of scaffolds: 9984\u001b[0m\n",
      "\u001b[32m2025-04-05 17:20:55.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mnumber of smiles: 25128\u001b[0m\n",
      "\u001b[32m2025-04-05 17:20:56.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of consolidation_fold0: 43183; length of train: 30287; length of valid: 6448; length of test: 6448\u001b[0m\n",
      "\u001b[32m2025-04-05 17:20:56.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of consolidation_fold1: 43568; length of train: 29902; length of valid: 6833; length of test: 6833\u001b[0m\n",
      "\u001b[32m2025-04-05 17:20:56.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of consolidation_fold2: 44247; length of train: 29223; length of valid: 7512; length of test: 7512\u001b[0m\n",
      "\u001b[32m2025-04-05 17:20:56.629\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of consolidation_fold3: 44472; length of train: 28998; length of valid: 7737; length of test: 7737\u001b[0m\n",
      "\u001b[32m2025-04-05 17:20:56.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of consolidation_fold4: 44940; length of train: 28530; length of valid: 8205; length of test: 8205\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "scaffold_split_df(data_consolidation, \"consolidation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987fbba89bf24d9a946ab8a8d4acdc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 17:21:06.484\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mnumber of scaffolds: 385\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:06.486\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mnumber of smiles: 792\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:06.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of cyanine_fold0: 1767; length of train: 1225; length of valid: 271; length of test: 271\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:06.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of cyanine_fold1: 1839; length of train: 1153; length of valid: 343; length of test: 343\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:06.572\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of cyanine_fold2: 1724; length of train: 1268; length of valid: 228; length of test: 228\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:06.600\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of cyanine_fold3: 1871; length of train: 1121; length of valid: 375; length of test: 375\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:06.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of cyanine_fold4: 1775; length of train: 1217; length of valid: 279; length of test: 279\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "scaffold_split_df(data_cyanine, \"cyanine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14218da86a18432bbf9f057f684f3899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-05 17:21:08.798\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mnumber of scaffolds: 278\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:08.799\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mnumber of smiles: 704\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:08.822\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of xanthene_fold0: 1377; length of train: 915; length of valid: 231; length of test: 231\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:08.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of xanthene_fold1: 1328; length of train: 964; length of valid: 182; length of test: 182\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:08.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of xanthene_fold2: 1307; length of train: 985; length of valid: 161; length of test: 161\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:08.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of xanthene_fold3: 1466; length of train: 826; length of valid: 320; length of test: 320\u001b[0m\n",
      "\u001b[32m2025-04-05 17:21:08.915\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mscaffold_split_df\u001b[0m:\u001b[36m35\u001b[0m - \u001b[1mlength of xanthene_fold4: 1398; length of train: 894; length of valid: 252; length of test: 252\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "scaffold_split_df(data_xanthene, \"xanthene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
